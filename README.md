# Active-Perception-using-Neural-Radiance-Fields
Siming He, Christopher D. Hsu∗, Dexter Ong∗, Yifei Simon Shao, Pratik Chaudhari

## Video of Active Perception in Habitat Simulation Scene
In each video, the third-person view and top view of active perception are shown on the left. The ground truth and NeRF synthesis of image, depth, and semantic segmentation in first-person view are shown on the right. During each trajectory, the NeRF synthesis result looks bad because the agent is moving to areas with higher predictive information (usually areas with less reconstruction quality). After each trajectory, the NeRF is trained on collected observations and the NeRF synthesis result becomes better.  
### Scene 1


https://github.com/grasp-lyrl/Active-Perception-using-Neural-Radiance-Fields/assets/69362937/ed965c50-9be2-4cf9-9d37-448d2e152865


### Scene 2


https://github.com/grasp-lyrl/Active-Perception-using-Neural-Radiance-Fields/assets/69362937/ef864e3b-68a0-4c4f-8a80-1ba5ed6642eb


### Scene 3


https://github.com/grasp-lyrl/Active-Perception-using-Neural-Radiance-Fields/assets/69362937/d1fce785-59c5-458a-8cc5-255a789043bf


